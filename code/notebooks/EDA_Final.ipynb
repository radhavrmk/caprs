{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref_colors(xlsx, refcolor_sheet):\n",
    "    print(\"\\n Loading Reference Colors ...\")\n",
    "    color_ref = xlsx.parse(refcolor_sheet)\n",
    "    color_array = color_ref[\"colorname0\"].unique()\n",
    "    print(color_array[:5])\n",
    "    return color_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categories(xlsx, category_sheet):\n",
    "    print(\"\\n Loading Category data ...\")\n",
    "    df_cat = xlsx.parse(category_sheet)\n",
    "    df_cat.dropna(inplace=True)\n",
    "    df_cat[\"Category ID\"] = df_cat[\"Category ID\"].astype(\"int32\")\n",
    "    df_cat= df_cat[[\"Category ID\",\"Category Name\"]]\n",
    "    print(df_cat.sample(5))\n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def map_category(x):\n",
    "    corrections = {\n",
    "                    \"Boots\": \"Boots & Booties\",\n",
    "                    \"T-shirts\":\"T Shirts\", \n",
    "                    \"Sports Bras\":\"Bras\",\n",
    "                    \"Denim\": \"Jeans\"\n",
    "                  }    \n",
    "    x = x.split(\":\")[0].strip()\n",
    "    mapped = corrections.get(x,0)\n",
    "    if corrections.get(x,0) != 0:\n",
    "        x = corrections.get(x,0)\n",
    "    return x\n",
    "\n",
    "def load_occasion_data(xlsx, occassion_sheet, catagory_df):\n",
    "\n",
    "    print(\"\\n Loading Occasion data ...\")\n",
    "    df_occassion = xlsx.parse(occassion_sheet, header=None, usecols=0)\n",
    "    df_occassion.columns=[\"category\"]\n",
    "    \n",
    "    \"\"\"Remove lines with only category name and no occasion mapping and blank lines\"\"\" \n",
    "    df_occassion[\"category\"] = df_occassion[df_occassion[\"category\"].map(lambda x : \":\" in x)]\n",
    "    df_occassion.dropna(inplace=True)\n",
    "    \n",
    "    \"\"\"Ensure category is one of the categories in Category-subcategory list\"\"\" \n",
    "    df_occassion[\"temp\"] = df_occassion[\"category\"].apply(lambda x : \" \".join([i.strip() for i in x.split(\":\")[1].strip().split(\",\")] ))\n",
    "    df_occassion[\"category\"] = df_occassion[\"category\"].apply(lambda x : map_category(x))\n",
    "    df_occassion[\"category\"] = df_occassion[\"category\"]\n",
    "\n",
    "    \"\"\"Create one row per occasion\"\"\"\n",
    "    df_occassion = df_occassion.join(\n",
    "        df_occassion.temp.str.split(expand=True).stack().reset_index(drop=True, level=1)\n",
    "        .rename(\"occassion\")).drop(\"temp\",axis=1)\n",
    "\n",
    "    df_occassion=pd.merge(df_occassion, catagory_df, left_on=\"category\", right_on=\"Category Name\", how=\"left\").\\\n",
    "        drop(\"Category Name\", axis=1).rename(columns = {\"Category ID\": \"category_id\"})\n",
    "\n",
    "    \"\"\"Categories \"Sweaters\", \"Sweatshirt\" and Suits will be dropped as there are no matching Category ID's\"\"\"\n",
    "    df_occassion.dropna(inplace=True)\n",
    "    df_occassion[\"category_id\"] = df_occassion[\"category_id\"].astype(\"int32\")\n",
    "    print(df_occassion.sample(5))\n",
    "    return df_occassion\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_brands(xlsx,brands_sheet):\n",
    "    print(\"\\n Loading Brands data ...\")\n",
    "    df_topbrands = xlsx.parse(brands_sheet)\n",
    "    df_topbrands = df_topbrands[\"brand_name\"].append(df_topbrands[\"brand_name_synonym\"]).map(lambda x : \"\".join(x.split()).lower()).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "    print(df_topbrands.sample(5))\n",
    "    return df_topbrands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_influencers(xlsx,influencer_sheet,influencers):\n",
    "\n",
    "    print(\"\\n Loading Influencer data ...\")\n",
    "\n",
    "    \"\"\" List of influencers to look for as \"\"\"\n",
    "    df_influencers = xlsx.parse(influencer_sheet)\n",
    "    df_influencers[\"user_id\"] = df_influencers[\"user_id\"].map(lambda x : x[-13:])\n",
    "    df_influencers[\"influencers\"] = df_influencers[\"style_who_inspiries\"].map(lambda x : [1 if re.search(i,x) else 0 for i in influencers ])\n",
    "    df_influencers[influencers] = pd.DataFrame(df_influencers[\"influencers\"].values.tolist(), index = df_influencers.index)\n",
    "    df_influencers.drop([\"style_who_inspiries\", \"influencers\"], axis =1, inplace=True)\n",
    "    df_influencers.fillna(0,inplace=True)\n",
    "    print(df_influencers.sample(5))\n",
    "    return df_influencers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_data(xlsx, user_sheet, influencers):\n",
    "\n",
    "    print(\"\\n Loading User data ...\")\n",
    "    df_users = xlsx.parse(user_sheet)\n",
    "\n",
    "    user_fields = [\n",
    "        \"user_id\",\n",
    "        \"style_age_range\",\n",
    "        \"style_age_range_group\",\n",
    "        \"items_in_wishlist\",\n",
    "        \"style_brands_selected\",\n",
    "        \"style_size_preference_none\",\n",
    "        \"style_size_preference_petite\",\n",
    "        \"style_size_preference_extra_long\",\n",
    "        \"style_size_preference_plus\",\n",
    "        \"style_size_preference_maternity\",\n",
    "        \"style_size_preference_skipped\",\n",
    "        \"style_vibe\",\n",
    "        \"has_stype_vibe\",\n",
    "        \"style_who_inspiries_skipped\",\n",
    "        \"style_looks_wanted_dates\",\n",
    "        \"style_looks_wanted_everyday\",\n",
    "        \"style_looks_wanted_formal\",\n",
    "        \"style_looks_wanted_nights\",\n",
    "        \"style_looks_wanted_other\",\n",
    "        \"style_looks_wanted_summer\",\n",
    "        \"style_looks_wanted_travel\",\n",
    "        'style_looks_wanted_winter',\n",
    "        \"style_looks_wanted_work\",\n",
    "        \"style_looks_wanted_workouts\",\n",
    "        \"style_looks_wanted_skipped\",\n",
    "        'style_most_important_active', \n",
    "        'style_most_important_any',\n",
    "        'style_most_important_beach', \n",
    "        'style_most_important_dress',\n",
    "        'style_most_important_bags', \n",
    "        'style_most_important_jeans',\n",
    "        'style_most_important_jump', \n",
    "        'style_most_important_nothing',\n",
    "        'style_most_important_outwear', \n",
    "        'style_most_important_pants',\n",
    "        'style_most_important_shoes', \n",
    "        'style_most_important_tops',\n",
    "        'style_most_important_skipped'\n",
    "        ]\n",
    "\n",
    "    user_fillna_zero_columns = [\n",
    "        \"style_size_preference_none\",\n",
    "        \"style_size_preference_petite\",\n",
    "        \"style_size_preference_extra_long\",\n",
    "        \"style_size_preference_plus\",\n",
    "        \"style_size_preference_maternity\",\n",
    "        \"style_looks_wanted_dates\",\n",
    "        \"style_looks_wanted_everyday\",\n",
    "        \"style_looks_wanted_formal\",\n",
    "        \"style_looks_wanted_nights\",\n",
    "        \"style_looks_wanted_other\",\n",
    "        \"style_looks_wanted_summer\",\n",
    "        \"style_looks_wanted_travel\",\n",
    "        'style_looks_wanted_winter',\n",
    "        \"style_looks_wanted_work\",\n",
    "        \"style_looks_wanted_workouts\",\n",
    "        'style_most_important_active', \n",
    "        'style_most_important_any',\n",
    "        'style_most_important_beach', \n",
    "        'style_most_important_dress',\n",
    "        'style_most_important_bags', \n",
    "        'style_most_important_jeans',\n",
    "        'style_most_important_jump', \n",
    "        'style_most_important_nothing',\n",
    "        'style_most_important_outwear', \n",
    "        'style_most_important_pants',\n",
    "        'style_most_important_shoes', \n",
    "        'style_most_important_tops',\n",
    "        'style_most_important_skipped'\n",
    "    ]\n",
    "\n",
    "    user_drop_columns = [\n",
    "        \"style_age_range\",\n",
    "        \"style_brands_selected\",\n",
    "        \"has_stype_vibe\",\n",
    "        \"style_who_inspiries_skipped\",\n",
    "        \"items_in_wishlist\"\n",
    "    ]\n",
    "\n",
    "    user_int_conversion_columns = [\n",
    "        \"style_age_range_group\",\n",
    "        \"style_size_preference_none\",\n",
    "        \"style_size_preference_petite\",\n",
    "        \"style_size_preference_extra_long\",\n",
    "        \"style_size_preference_plus\",\n",
    "        \"style_size_preference_maternity\",\n",
    "        \"style_size_preference_skipped\",\n",
    "        \"style_looks_wanted_dates\",\n",
    "        \"style_looks_wanted_everyday\",\n",
    "        \"style_looks_wanted_formal\",\n",
    "        \"style_looks_wanted_nights\",\n",
    "        \"style_looks_wanted_other\",\n",
    "        \"style_looks_wanted_summer\",\n",
    "        \"style_looks_wanted_travel\",\n",
    "        'style_looks_wanted_winter',\n",
    "        \"style_looks_wanted_work\",\n",
    "        \"style_looks_wanted_workouts\",\n",
    "        \"style_looks_wanted_skipped\",\n",
    "        'style_most_important_active', \n",
    "        'style_most_important_any',\n",
    "        'style_most_important_beach', \n",
    "        'style_most_important_dress',\n",
    "        'style_most_important_bags', \n",
    "        'style_most_important_jeans',\n",
    "        'style_most_important_jump', \n",
    "        'style_most_important_nothing',\n",
    "        'style_most_important_outwear', \n",
    "        'style_most_important_pants',\n",
    "        'style_most_important_shoes', \n",
    "        'style_most_important_tops',\n",
    "        'style_most_important_skipped'\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    placeholder to impute style looks wanted columns\n",
    "    for now drop the columns\n",
    "    \"\"\"\n",
    "    style_looks_columns = [\n",
    "        \"style_looks_wanted_dates\",\n",
    "        \"style_looks_wanted_everyday\",\n",
    "        \"style_looks_wanted_formal\",\n",
    "        \"style_looks_wanted_nights\",\n",
    "        \"style_looks_wanted_other\",\n",
    "        \"style_looks_wanted_summer\",\n",
    "        \"style_looks_wanted_travel\",\n",
    "        'style_looks_wanted_winter',\n",
    "        \"style_looks_wanted_work\",\n",
    "        \"style_looks_wanted_workouts\",\n",
    "    ] \n",
    "\n",
    "    df_users = df_users[user_fields]\n",
    "    df_users[\"user_id\"] = df_users[\"user_id\"].map(lambda x : x[-13:])\n",
    "    df_users[\"style_age_range_group\"] = df_users[\"style_age_range_group\"].fillna(5)\n",
    "    df_users[\"style_vibe\"] = df_users[\"style_vibe\"].fillna(\"None\")\n",
    "\n",
    "    df_users[user_fillna_zero_columns] = df_users[user_fillna_zero_columns].fillna(0)\n",
    "    df_users[user_int_conversion_columns] = df_users[user_int_conversion_columns].astype(\"int64\")\n",
    "\n",
    "    \"\"\" Appending influencer data\"\"\"\n",
    "    df_users = pd.merge(df_users,df_influencers,left_on=\"user_id\",right_on=\"user_id\", how=\"left\")\n",
    "    df_users[influencers] = df_users[influencers].fillna(0).astype(\"int64\")\n",
    "\n",
    "    \"\"\"Duplicates are issue to be adressed later - revisit this\"\"\"\n",
    "    #df_users.duplicated(df_users.columns[1:]).sum()\n",
    "\n",
    "    df_users.drop(user_drop_columns + style_looks_columns + [\"style_vibe\"], axis=1, inplace=True)\n",
    "\n",
    "    print(df_users.sample(5))\n",
    "    return df_users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_item_data(xlsx, items_sheet, df_cat, df_brands, df_colors):\n",
    "    print(\"\\n Loading Item data ...\")\n",
    "    item_columns_tokeep = [\n",
    "            'user_id', 'brand_id', 'user_provided_brand_name', 'parsed_brand_name',\n",
    "            'store_id', 'user_provided_store_name','parsed_store_name','product_id', \n",
    "            'item_name_lower', 'product_category_id', 'paid_price',\n",
    "            'list_price', 'sale_price',\n",
    "            'order_total_amt', 'size', 'email_dt', 'color_parsed']\n",
    "\n",
    "    \"\"\" usecols not working. Seems to be a bug in the library. To be checked further\"\"\"\n",
    "    df_items = xlsx.parse(items_sheet)\n",
    "    df_items = df_items[item_columns_tokeep]\n",
    "\n",
    "\n",
    "    df_items[\"user_id\"] = df_items[\"user_id\"].map(lambda x : x[-13:])\n",
    "\n",
    "    \"\"\"Dropping rows with null product ID\"\"\"\n",
    "    df_items = df_items[~df_items[\"product_id\"].isnull()]\n",
    "    df_items[\"product_id\"] = df_items[\"product_id\"].map(lambda x : x[-8:])\n",
    "\n",
    "\n",
    "    df_items[\"product_category_id\"].fillna(0, inplace=True)\n",
    "    df_items[\"product_category_id\"] = df_items[\"product_category_id\"].astype(\"int64\")\n",
    "\n",
    "\n",
    "    \"\"\"Adding \"on sale\" column indicate a purchase of item in sale \"\"\"\n",
    "    df_items[\"on_sale\"] = df_items[\"sale_price\"].map(lambda x : True if x > 0 else False)\n",
    "    df_items[\"part_of_order\"] = df_items.apply(lambda x : True if x[\"order_total_amt\"] > x[\"paid_price\"] else False, axis=1)\n",
    "\n",
    "    \"\"\"Get brand id with blank from other two brand columns when avaialble, else set to None\"\"\"\n",
    "    df_items[\"brand_id\"] = df_items[\"brand_id\"].fillna(df_items[\"parsed_brand_name\"])\n",
    "    df_items[\"brand_id\"] = df_items[\"brand_id\"].fillna(df_items[\"user_provided_brand_name\"])\n",
    "    condition = pd.notnull(df_items[\"brand_id\"])\n",
    "    df_items.loc[condition,\"brand_id\"] = df_items.loc[condition,\"brand_id\"].map(lambda x : \"\".join(x.split()).lower())\n",
    "    df_items[\"brand_id\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "    \"\"\"Get store id with blank from other two brand columns when avaialble, else set to None\"\"\"\n",
    "    df_items[\"store_id\"] = df_items[\"store_id\"].fillna(df_items[\"parsed_store_name\"])\n",
    "    df_items[\"store_id\"] = df_items[\"store_id\"].fillna(df_items[\"user_provided_store_name\"])\n",
    "    condition = pd.notnull(df_items[\"brand_id\"])\n",
    "    df_items.loc[condition,\"store_id\"] = df_items.loc[condition,\"store_id\"].map(lambda x : \"\".join(x.split()).lower())\n",
    "\n",
    "    \"\"\"Set store id ro Brand ID when null\"\"\"\n",
    "    df_items[\"store_id\"] = df_items[\"store_id\"].fillna(df_items[\"brand_id\"])\n",
    "\n",
    "    \"\"\"Set store ID to none if still not available\"\"\"\n",
    "    df_items[\"store_id\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "\n",
    "    \"\"\"Generate a new column that flags whether a store(brand) is a top rand \"\"\"\n",
    "    df_items[\"top_brand\"] = df_items[\"store_id\"].map(lambda x : True if x in df_brands.values.tolist() else False )\n",
    "\n",
    "\n",
    "    \"\"\" Generate color column : pick value from color_parsed, if not available check if color is mentioned\n",
    "    in Item description. In both the cases, match against master color list \n",
    "    \"\"\"\n",
    "    df_items[\"item_name_lower\"] = df_items[\"item_name_lower\"].fillna(\"None\")\n",
    "    df_items[\"item_name_lower\"] = df_items[\"item_name_lower\"].map(lambda x : str(x).lower())\n",
    "\n",
    "    df_items[\"color_parsed\"] = df_items[\"color_parsed\"].fillna(\"None\")\n",
    "    df_items[\"color_parsed\"] = df_items[\"color_parsed\"].map(lambda x : str(x).lower())\n",
    "\n",
    "\n",
    "    color_parse_lst = [set(str(e)) for e in df_items[\"color_parsed\"].str.split()]\n",
    "    df_items[\"color\"] = [e&set(df_colors) for e in color_parse_lst]\n",
    "    df_items[\"color\"] = [list(e)[0] if len(list(e))!=0 else \"\" for e in df_items[\"color\"]]\n",
    "\n",
    "\n",
    "    color_ntlst = [set(e) for e in df_items[\"item_name_lower\"].str.split()]\n",
    "    df_items[\"color_tmp1\"] = [e&set(df_colors) for e in color_ntlst]\n",
    "    df_items[\"color_tmp1\"] = [list(e)[0] if len(list(e))!=0 else \"\" for e in df_items[\"color_tmp1\"]]\n",
    "\n",
    "    df_items[\"color\"] = df_items[\"color\"].replace('', df_items[\"color_tmp1\"])\n",
    "\n",
    "    \"\"\"Generate bins for price\"\"\"\n",
    "    bins = np.array([0.0, 20.0, 50.0, 100.0, 200.0, 500.0, 1000.0, 20000.0])\n",
    "    df_items[\"price_bin\"] = np.digitize(df_items[\"paid_price\"], bins)\n",
    "\n",
    "\n",
    "\n",
    "    item_drop_columns = [\n",
    "        \"sale_price\",\n",
    "        \"order_total_amt\",\n",
    "        \"user_provided_brand_name\",\n",
    "        \"parsed_brand_name\",\n",
    "        \"user_provided_store_name\",\n",
    "        \"parsed_store_name\",\n",
    "        \"email_dt\",\n",
    "        \"list_price\",\n",
    "        \"color_tmp1\",\n",
    "        \"color_parsed\"\n",
    "    ]\n",
    "    df_items.drop(item_drop_columns, axis = 1,inplace=True)\n",
    "\n",
    "    \"\"\"Keep only if product elongs to master category list\"\"\"\n",
    "    condition = df_items.product_category_id.map( lambda x : x in df_cat[\"Category ID\"].values.tolist())\n",
    "    df_items = df_items[condition]\n",
    "\n",
    "    \"\"\" Drop duplicates. This is an issue, to e explored further on why there are duplicates\"\"\"\n",
    "    df_items = df_items.drop_duplicates()\n",
    "\n",
    "    \"\"\" Warning : Hard coding here. Eliminate Cosmetics and Kids clothes categories\"\"\"\n",
    "    df_items = df_items[df_items[\"product_category_id\"] <500]\n",
    "\n",
    "    \"\"\"Generate column combining category ID and Category Name \"\"\"\n",
    "    df_items = pd.merge(df_items,df_cat, left_on=\"product_category_id\", right_on=\"Category ID\", how=\"left\",)\n",
    "    df_items.product_category_id = df_items.apply(lambda x : str(x[\"product_category_id\"]) + \" - \"+ str(x[\"Category Name\"]), axis=1)\n",
    "\n",
    "    df_items.drop(\"Category ID\", axis =1, inplace=True)\n",
    "\n",
    "\n",
    "    print(df_items.sample(5))\n",
    "    return df_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet Names in the input XLSX:\n",
      " ['1. top_100_brands(brandname+syn', '2. reference_color', '3. personalization_rules(exampl', '4. sample_occasion', '5. influencer_color_rules', '6. category_and_subcategory', '7. user_subset', '8. item_subset', '9. category_ids', '10. brands_affinity', '11. styling_segments', '12. wishlist_items', '13. user_influencer', '14. 100_users_set', '15. 100_users_item_set', '16. 100_user_influencer']\n",
      "\n",
      " Loading Reference Colors ...\n",
      "['white' 'aliceblue' 'antiquewhite' 'aquamarine' 'azure']\n",
      "\n",
      " Loading Category data ...\n",
      "    Category ID Category Name\n",
      "66          480       Wallets\n",
      "62          440          Hats\n",
      "70          520         Nails\n",
      "28          155       Hoodies\n",
      "53          320      Shoulder\n",
      "\n",
      " Loading Occasion data ...\n",
      "    category occassion  category_id\n",
      "214     Long     Night          132\n",
      "47     Flats    Movies          220\n",
      "139   Formal    Formal          144\n",
      "88    Skirts    Dinner          121\n",
      "53   Blouses      Life          111\n",
      "\n",
      " Loading Brands data ...\n",
      "113                venus\n",
      "60                  loft\n",
      "61           lord&taylor\n",
      "0      abercrombie&fitch\n",
      "74              nastygal\n",
      "dtype: object\n",
      "\n",
      " Loading Influencer data ...\n",
      "          user_id  ariellecharnas  blaireadiebee  blakevond  chiaraferragni  \\\n",
      "66  1538503539686               0              0          0               1   \n",
      "4   1528762526242               0              0          0               1   \n",
      "48  1533238049769               0              1          0               0   \n",
      "72  1533737578164               0              0          0               0   \n",
      "47  1539277708892               0              1          0               1   \n",
      "\n",
      "    hannahbronfman  jordynwoods  manrepeller  mayemusk  nicolettemason  \\\n",
      "66               1            0            0         0               0   \n",
      "4                0            0            0         0               0   \n",
      "48               0            0            0         0               0   \n",
      "72               0            0            0         0               1   \n",
      "47               0            0            1         0               1   \n",
      "\n",
      "    seaofshoes  somethingnavy  weworewhat  \n",
      "66           0              0           1  \n",
      "4            0              0           0  \n",
      "48           0              0           0  \n",
      "72           0              0           0  \n",
      "47           0              1           0  \n",
      "\n",
      " Loading User data ...\n",
      "           user_id  style_age_range_group  style_size_preference_none  \\\n",
      "6    1529614244802                      2                           0   \n",
      "54   1547183116675                      5                           0   \n",
      "42   1540519065537                      1                           0   \n",
      "1    1539198476327                      0                           0   \n",
      "115  1514483609267                      5                           0   \n",
      "\n",
      "     style_size_preference_petite  style_size_preference_extra_long  \\\n",
      "6                               0                                 0   \n",
      "54                              0                                 0   \n",
      "42                              0                                 0   \n",
      "1                               0                                 0   \n",
      "115                             0                                 0   \n",
      "\n",
      "     style_size_preference_plus  style_size_preference_maternity  \\\n",
      "6                             0                                0   \n",
      "54                            1                                0   \n",
      "42                            0                                0   \n",
      "1                             0                                0   \n",
      "115                           0                                0   \n",
      "\n",
      "     style_size_preference_skipped  style_looks_wanted_skipped  \\\n",
      "6                                0                           0   \n",
      "54                               0                           1   \n",
      "42                               0                           0   \n",
      "1                                0                           0   \n",
      "115                              1                           1   \n",
      "\n",
      "     style_most_important_active     ...      blakevond  chiaraferragni  \\\n",
      "6                              0     ...              0               0   \n",
      "54                             0     ...              0               0   \n",
      "42                             0     ...              0               1   \n",
      "1                              0     ...              0               1   \n",
      "115                            0     ...              0               0   \n",
      "\n",
      "     hannahbronfman  jordynwoods  manrepeller  mayemusk  nicolettemason  \\\n",
      "6                 0            0            0         0               0   \n",
      "54                0            0            0         0               0   \n",
      "42                0            0            0         0               0   \n",
      "1                 0            0            0         0               0   \n",
      "115               0            0            0         0               0   \n",
      "\n",
      "     seaofshoes  somethingnavy  weworewhat  \n",
      "6             0              0           1  \n",
      "54            0              0           0  \n",
      "42            0              0           0  \n",
      "1             0              0           1  \n",
      "115           0              0           0  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "\n",
      " Loading Item data ...\n",
      "            user_id   brand_id   store_id product_id  \\\n",
      "4468  1506291233259       asos       asos   643b095c   \n",
      "3537  1544840725508     fallon  nordstrom   7caa6dec   \n",
      "2413  1528762526242  forever21  forever21   4655581c   \n",
      "6952  1532180138666    oldnavy    oldnavy   35348475   \n",
      "1321  1504447154564      boden      boden   280248fa   \n",
      "\n",
      "                                 item_name_lower product_category_id  \\\n",
      "4468  asos shorts in mono print with pom pom hem        122 - Shorts   \n",
      "3537                              pave band ring       410 - Jewelry   \n",
      "2413                    faux stone cocktail ring       410 - Jewelry   \n",
      "6952                 plaid swing shirt dress for        141 - Casual   \n",
      "1321                       aurelia ottoman dress        141 - Casual   \n",
      "\n",
      "      paid_price   size  on_sale  part_of_order  top_brand color  price_bin  \\\n",
      "4468       20.00   UK 8    False          False       True                2   \n",
      "3537       22.97    NaN    False          False      False                2   \n",
      "2413        5.00    NaN    False          False       True                1   \n",
      "6952       36.99     XS    False          False       True                2   \n",
      "1321      133.00  4 PET    False          False       True                4   \n",
      "\n",
      "     Category Name  \n",
      "4468        Shorts  \n",
      "3537       Jewelry  \n",
      "2413       Jewelry  \n",
      "6952        Casual  \n",
      "1321        Casual  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    excel_data = pd.ExcelFile('../../data/raw/mar19.xlsx')\n",
    "    print(\"Sheet Names in the input XLSX:\\n\",excel_data.sheet_names)\n",
    "\n",
    "    \"\"\"Variables to avoid hardcoding of sheet names which might change later\"\"\"\n",
    "    top_brands_sheet = \"1. top_100_brands(brandname+syn\"\n",
    "    category_sheet = \"9. category_ids\"\n",
    "    user_sheet = \"14. 100_users_set\"\n",
    "    items_sheet = \"15. 100_users_item_set\"\n",
    "    wishlist_sheet = \"12. wishlist_items\"\n",
    "    influencer_sheet = \"16. 100_user_influencer\"\n",
    "    refcolor_sheet = \"2. reference_color\"\n",
    "    occassion_sheet = '6. category_and_subcategory'\n",
    "\n",
    "    ''' hardcoded as influencers are mixed together as one long string in the data witout seperators'''\n",
    "    influencers = [\n",
    "        \"ariellecharnas\",\n",
    "        \"blaireadiebee\",\n",
    "        \"blakevond\",\n",
    "        \"chiaraferragni\",\n",
    "        \"hannahbronfman\",\n",
    "        \"jordynwoods\",\n",
    "        \"manrepeller\",\n",
    "        \"mayemusk\",\n",
    "        \"nicolettemason\",\n",
    "        \"seaofshoes\",\n",
    "        \"somethingnavy\",\n",
    "        \"weworewhat\"\n",
    "    ]\n",
    "\n",
    "    \"\"\" Load and clean data \"\"\"\n",
    "    color_array = load_ref_colors(excel_data, refcolor_sheet)\n",
    "    df_category = load_categories(excel_data, category_sheet)\n",
    "    df_occassion = load_occasion_data(excel_data, occassion_sheet, df_category)\n",
    "    df_topbrands = load_brands(excel_data, top_brands_sheet)\n",
    "    df_influencers = load_influencers(excel_data,influencer_sheet, influencers)\n",
    "    df_users = load_user_data(excel_data,user_sheet, influencers)\n",
    "    df_items = load_item_data(excel_data, items_sheet, df_category, df_topbrands, color_array)\n",
    "\n",
    "    ## Write dataframe to CSV file\n",
    "    df_users.to_csv(\"../../data/processed/users.csv\",index=False)\n",
    "    df_items.to_csv(\"../../data/processed/items.csv\",index=False)\n",
    "    df_influencers.to_csv(\"../../data/processed/influencers.csv\",index=False)\n",
    "    df_category.to_csv(\"../../data/processed/categories.csv\",index=False)\n",
    "    df_occassion.to_csv(\"../../data/processed/occassions.csv\",index=False)\n",
    "    excel_data = None\n",
    "    df_users = None\n",
    "    df_items = None\n",
    "    df_influencers = None\n",
    "    df_category = None\n",
    "    df_occassion = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
